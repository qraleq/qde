@Article{shannon1949,
	Author         = "C. E. Shannon",
	Title          = "Communication in the Presence of Noise",
	Journal        = "Proc. Institute of Radio Engineers",
	Volume         = "37",
	Number         = "1",
	Pages          = "10--21",
	localfile      = "file:///d0/bosmanoglu/ownCloud/Papers/Shannon, C.
	E./communication in the presence of noise.pdf",
	year           = 1949
}

@article{nyquist1928certain,
	title={Certain topics in telegraph transmission theory},
	author={Nyquist, Harry},
	journal={Transactions of the American Institute of Electrical Engineers},
	volume={47},
	number={2},
	pages={617--644},
	year={1928},
	publisher={IEEE}
}


@article{Candes2007,
abstract = {We consider the problem of reconstructing a sparse signal {\$}x{\^{}}0\backslashin\backslashR{\^{}}n{\$} from a limited number of linear measurements. Given {\$}m{\$} randomly selected samples of {\$}U x{\^{}}0{\$}, where {\$}U{\$} is an orthonormal matrix, we show that {\$}\backslashell{\_}1{\$} minimization recovers {\$}x{\^{}}0{\$} exactly when the number of measurements exceeds $\backslash$[ m$\backslash$geq $\backslash$mathrm{\{}Const{\}}$\backslash$cdot$\backslash$mu{\^{}}2(U)$\backslash$cdot S$\backslash$cdot$\backslash$log n, $\backslash$] where {\$}S{\$} is the number of nonzero components in {\$}x{\^{}}0{\$}, and {\$}\backslashmu{\$} is the largest entry in {\$}U{\$} properly normalized: {\$}\backslashmu(U) = \backslashsqrt{\{}n{\}} \backslashcdot \backslashmax{\_}{\{}k,j{\}} |U{\_}{\{}k,j{\}}|{\$}. The smaller {\$}\backslashmu{\$}, the fewer samples needed. The result holds for ``most'' sparse signals {\$}x{\^{}}0{\$} supported on a fixed (but arbitrary) set {\$}T{\$}. Given {\$}T{\$}, if the sign of {\$}x{\^{}}0{\$} for each nonzero entry on {\$}T{\$} and the observed values of {\$}Ux{\^{}}0{\$} are drawn at random, the signal is recovered with overwhelming probability. Moreover, there is a sense in which this is nearly optimal since any method succeeding with the same probability would require just about this many samples.},
archivePrefix = {arXiv},
arxivId = {math/0611957},
author = {Cand{\`{e}}s, Emmanuel and Romberg, Justin},
doi = {10.1088/0266-5611/23/3/008},
eprint = {0611957},
file = {:Y$\backslash$:/Literature/Compressive Sensing/Sparsity and Incoherence in Compressive Sampling.pdf:pdf},
isbn = {0266-5611},
issn = {0266-5611},
journal = {Inverse Problems},
keywords = {1 -minimization,basis pursuit,discrete fourier transform,matrices,restricted orthonormality,singular values of random,sparsity,wavelets},
number = {3},
pages = {969--985},
primaryClass = {math},
title = {{Sparsity and incoherence in compressive sampling}},
volume = {23},
year = {2007}
}


@article{Candes2006,
	abstract = {This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f{\&}isin;CN and a randomly chosen set of frequencies {\&}Omega;. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set {\&}Omega;? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)={\&}sigma;{\&}tau;{\&}isin;Tf({\&}tau;){\&}delta;(t-{\&}tau;) obeying |T|{\&}le;CM{\&}middot;(log N)-1 {\&}middot; |{\&}Omega;| for some constant CM{\textgreater}0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N-M), f can be reconstructed exactly as the solution to the {\&}{\#}8467;1 minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for CM which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|{\&}middot;logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N-M) would in general require a number of frequency samples at least proportional to |T|{\&}middot;logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.},
	archivePrefix = {arXiv},
	arxivId = {math/0409186},
	author = {Cand{\`{e}}s, Emmanuel J. and Romberg, Justin and Tao, Terence},
	doi = {10.1109/TIT.2005.862083},
	eprint = {0409186},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Candes, Romberg, Tao - Robust Uncertainty Principles.pdf:pdf},
	isbn = {0018-9448 VO - 52},
	issn = {00189448},
	journal = {IEEE Transactions on Information Theory},
	keywords = {Convex optimization,Duality in optimization,Free probability,Image reconstruction,Linear programming,Random matrices,Sparsity,Total-variation minimization,Trigonometric expansions,Uncertainty principle},
	number = {2},
	pages = {489--509},
	pmid = {1580791},
	primaryClass = {math},
	title = {{Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information}},
	volume = {52},
	year = {2006}
}

@article{Candes2006_2,
	abstract = {Conventional wisdom and common practice in acquisition and reconstruction of images from frequency data follow the basic principle of the Nyquist density sampling theory. This principle states that to reconstruct an image, the number of Fourier samples we need to acquire must match the desired resolution of the image, i.e. the number of pixels in the image. This paper surveys an emerging theory which goes by the name of “compressive sampling” or “compressed sensing,” and which says that this conventional wisdom is inaccurate. Perhaps surprisingly, it is possible to reconstruct images or signals of scientiﬁc interest accurately and sometimes even exactly from a number of samples which is far smaller than the desired resolution of the image/signal, e.g. the number of pixels in the image. It is believed that compressive sampling has far reaching implications. For example, it suggests the possibility of new data acquisition protocols that translate analog information into digital form with fewer sensors than what was considered necessary. This new sampling theory may come to underlie procedures for sampling and compressing data simultaneously. In this short survey, we provide some of the key mathematical insights underlying this new theory, and explain some of the interactions between compressive sampling and other ﬁelds such as statistics, information theory, coding theory, and theoretical computer science.},
	author = {Cand{\`{e}}s, Emmanuel J.},
	doi = {10.4171/022-3/69},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Candes - Compressive Sampling.pdf:pdf},
	isbn = {978-3-03719-022-7},
	issn = {1095-9114},
	journal = {Proceedings of the International Congress of Mathematicians},
	keywords = {1 -minimization,compressive sampling,error cor-,linear programming,signal recovery,sparsity,systems of linear equations,underdertermined,uniform uncertainty principle},
	pages = {1433--1452},
	pmid = {19923686},
	title = {{Compressive sampling}},
	year = {2006}
}

@article{Candes2008,
	abstract = {Conventional approaches to sampling signals or images follow Shannon's theorem: the sampling rate must be at least twice the maximum frequency present in the signal (Nyquist rate). In the field of data conversion, standard analog-to-digital converter (ADC) technology implements the usual quantized Shannon representation - the signal is uniformly sampled at or above the Nyquist rate. This article surveys the theory of compressive sampling, also known as compressed sensing or CS, a novel sensing/sampling paradigm that goes against the common wisdom in data acquisition. CS theory asserts that one can recover certain signals and images from far fewer samples or measurements than traditional methods use.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1307.1360v1},
	author = {Candes, E.J. and Wakin, M.B.},
	doi = {10.1109/MSP.2007.914731},
	eprint = {arXiv:1307.1360v1},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/An Introduction To Compressive Sensing.pdf:pdf},
	isbn = {1053-5888 VO - 25},
	issn = {1053-5888},
	journal = {IEEE Signal Processing Magazine},
	number = {2},
	pages = {21--30},
	pmid = {4472240},
	title = {{An Introduction To Compressive Sampling}},
	volume = {25},
	year = {2008}
}

@article{Candes2006,
	abstract = {Suppose we are given a vector f in a class FsubeRopfN , e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr2) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|(n)lesRmiddotn-1p/, where R{\textgreater}0 and p{\textgreater}0. Suppose that we take measurements yk=langf{\#} ,Xkrang,k=1,...,K, where the Xk are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0{\textless}p{\textless}1 and with overwhelming probability, our reconstruction ft, defined as the solution to the constraints yk=langf{\#} ,Xkrang with minimal lscr1 norm, obeys parf-f{\#}parlscr2lesCp middotRmiddot(K/logN)-r, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed},
	archivePrefix = {arXiv},
	arxivId = {math/0410542},
	author = {Candes, Emmanuel J. and Tao, Terence},
	doi = {10.1109/TIT.2006.885507},
	eprint = {0410542},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Near-Optimal Signal Recovery From Random Projections - Universal Encoding Strategies.pdf:pdf},
	isbn = {0018-9448},
	issn = {00189448},
	journal = {IEEE Transactions on Information Theory},
	keywords = {Concentration of measure,Convex optimization,Duality in optimization,Linear programming,Random matrices,Random projections,Signal recovery,Singular values of random matrices,Sparsity,Trigonometric expansions,Uncertainty principle},
	number = {12},
	pages = {5406--5425},
	pmid = {4016283},
	primaryClass = {math},
	title = {{Near-optimal signal recovery from random projections: Universal encoding strategies?}},
	volume = {52},
	year = {2006}
}

@article{Donoho2006,
	abstract = {Suppose x is an unknown vector in R-m (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n = O(m(1/4) log(5/2)(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an l(p), ball for O {\textless} p {\textless}= 1. The N most important coefficients in that expansion allow reconstruction with l(2) error O(N1/2-1/p). It is possible to design n = O (N log (m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of "random" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of l(p) balls in high-dimensional Euclidean space in the case 0 {\textless} p {\textless}= 1, and give a criterion identifying near-optimal subspaces for Gel'fand n-widths. We show that "most" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces.},
	archivePrefix = {arXiv},
	arxivId = {1204.4227v1},
	author = {Donoho, D L},
	doi = {Doi 10.1109/Tit.2006.871582},
	eprint = {1204.4227v1},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Donoho - Compressed Sensing.pdf:pdf;:Y$\backslash$:/Literature/Compressive Sensing/Compressive Sensing Applications/CSMRI.pdf:pdf},
	isbn = {0018-9448},
	issn = {00189448},
	journal = {IEEE Transactions on Information Theory},
	keywords = {adaption,adaptive sampling,almost-spherical sections of banach spaces,approximation,bases,basis pursuit,eigenvalues of random matrices,entropy numbers,gel'fand n-widths,information-based complexity,integrated sensing and processing,linear-operators,minimum l(1)-norm decomposition,noise,optimal recovery,quotient-of-a-subspace theorem,representations,spaces,sparse solution of linear equations},
	number = {4},
	pages = {1289--1306},
	pmid = {1614066},
	title = {{Compressed sensing}},
	volume = {52},
	year = {2006}
}

@article{Baraniuk2007,
	abstract = {This lecture note presents a new method to capture and represent compressible signals at a rate significantly below the Nyquist rate. This method, called compressive sensing, employs nonadaptive linear projections that preserve the structure of the signal; the signal is then reconstructed from these projections using an optimization process.},
	author = {Baraniuk, R.G.},
	doi = {10.1109/MSP.2007.4286571},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Baraniuk - Compressive Sensing.pdf:pdf},
	isbn = {1053-5888 VO - 24},
	issn = {1053-5888},
	journal = {IEEE Signal Processing Magazine},
	number = {July},
	pages = {118--121},
	pmid = {19158952},
	title = {{Compressive Sensing [Lecture Notes]}},
	volume = {24},
	year = {2007}
}

@article{Lustig2008,
	abstract = {The reconstruction of signals and images from significantly fewer measurements can be realized under the compressed sensing (CS) paradigm. Compressed sensing can be done through magnetic resonance imaging (MRI) and has been studied just recently. The study reviewed the requirements for successful CS, described their natural fit to MRI, and then give examples of four interesting applications of CS in MRI. The investigation of CS-MRI showed such challenges as optimizing sampling trajectories, developing improved sparse transforms that are incoherent to the sampling operator, studying reconstruction quality in terms of clinical significance, and improving the speed of reconstruction algorithms.},
	archivePrefix = {arXiv},
	arxivId = {1204.4227v1},
	author = {Lustig, Michael and Donoho, David L. and Santos, Juan M. and Pauly, John M.},
	doi = {10.1109/MSP.2007.914728},
	eprint = {1204.4227v1},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Compressive Sensing Applications/CSMRI.pdf:pdf},
	isbn = {0018-9448},
	issn = {10535888},
	journal = {IEEE Signal Processing Magazine},
	number = {2},
	pages = {72--82},
	pmid = {1614066},
	title = {{Compressed sensing MRI: A look at how CS can improve on current imaging techniques}},
	volume = {25},
	year = {2008}
}

@article{duarte2008single,
	title={Single-pixel imaging via compressive sampling},
	author={Duarte, Marco F and Davenport, Mark A and Takbar, Dharmpal and Laska, Jason N and Sun, Ting and Kelly, Kevin F and Baraniuk, Richard G},
	journal={IEEE signal processing magazine},
	volume={25},
	number={2},
	pages={83--91},
	year={2008},
	publisher={IEEE}
}

@article{Tropp2010,
	abstract = {Wideband analog signals push contemporary analog-to-digital conversion systems to their performance limits. In many applications, however, sampling at the Nyquist rate is inefficient because the signals of interest contain only a small number of significant frequencies relative to the bandlimit, although the locations of the frequencies may not be known a priori. For this type of sparse signal, other sampling strategies are possible. This paper describes a new type of data acquisition system, called a random demodulator, that is constructed from robust, readily available components. Let K denote the total number of frequencies in the signal, and let W denote its bandlimit in Hz. Simulations suggest that the random demodulator requires just O(K log(W/K)) samples per second to stably reconstruct the signal. This sampling rate is exponentially lower than the Nyquist rate of W Hz. In contrast with Nyquist sampling, one must use nonlinear methods, such as convex programming, to recover the signal from the samples taken by the random demodulator. This paper provides a detailed theoretical analysis of the system's performance that supports the empirical observations.},
	archivePrefix = {arXiv},
	arxivId = {0902.0026},
	author = {Tropp, Joel A. and Laska, Jason N. and Duarte, Marco F. and Romberg, Justin K. and Baraniuk, Richard G.},
	doi = {10.1109/TIT.2009.2034811},
	eprint = {0902.0026},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/0902.0026.pdf:pdf},
	isbn = {0018-9448},
	issn = {00189448},
	journal = {IEEE Transactions on Information Theory},
	keywords = {Analog-to-digital conversion,Compressive sampling,Sampling theory,Signal recovery,Sparse approximation},
	number = {1},
	pages = {520--544},
	pmid = {5361485},
	title = {{Beyond Nyquist: Efficient sampling of sparse bandlimited signals}},
	volume = {56},
	year = {2010}
}

@article{Duarte2011,
	abstract = {Compressed sensing (CS) is an emerging field that has attracted considerable research interest over the past few years. Previous review articles in CS limit their scope to standard discrete-to-discrete measurement architectures using matrices of randomized nature and signal models based on standard sparsity. In recent years, CS has worked its way into several new application areas. This, in turn, necessitates a fresh look on many of the basics of CS. The random matrix measurement operator must be replaced by more structured sensing architectures that correspond to the characteristics of feasible acquisition hardware. The standard sparsity prior has to be extended to include a much richer class of signals and to encode broader data models, including continuous-time signals. In our overview, the theme is exploiting signal and measurement structure in compressive sensing. The prime focus is bridging theory and practice; that is, to pinpoint the potential of structured CS strategies to emerge from the math to the hardware. Our summary highlights new directions as well as relations to more traditional CS, with the hope of serving both as a review to practitioners wanting to join this emerging field, and as a reference for researchers that attempts to put some of the existing ideas in perspective of practical applications.},
	archivePrefix = {arXiv},
	arxivId = {1106.6224},
	author = {Duarte, Marco F. and Eldar, Yonina C},
	doi = {10.1109/TSP.2011.2161982},
	eprint = {1106.6224},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Structured Compressed Sensing - From Theory To Applications.pdf:pdf},
	month = {jun},
	number = {170},
	pages = {4053--4085},
	title = {{Structured Compressed Sensing: From Theory to Applications}},
	volume = {59},
	year = {2011}
}

@book{Rish2015,
	author = {Rish, Irina and Grabarnik, Genady},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Sparse Modeling{\_} Theory, Algorithms, and Applications [Rish {\&} Grabarnk 2014-12-05].pdf:pdf},
	isbn = {9781439828700},
	title = {{Sparse Modeling: Theory, Algorithms, and Applications}},
	year = {2015}
}


@Misc{romberg,
	author = {Justin Romberg},
	title = {Tsinghua {C}ourse on {S}parse {A}pproximation - Lecture notes},
	year = {2013},
	url = {http://jrom.ece.gatech.edu/tsinghua-oct13/},
	note = {{A}ccessed on 11.10.2017.}
}

@article{Candes2005,
	annote = {NULL},
	author = {Candes, Emmanuel and Tao, Terence},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Candes, Tao - Decoding by Linear Programming.pdf:pdf},
	journal = {IEEE Trans. Inf. Theory},
	number = {12},
	pages = {4203--4215},
	title = {{Decoding by Linear Programming}},
	volume = {51},
	year = {2005}
}

@book{Mallat2004,
	abstract = {This study reviewed the trabeculectomies (TEs) carried out in University Malaya Medical Center between 1994 to 1998. One hundred and nine of 132 eyes operated were in the primary glaucoma group of which 63 (47.7{\%}) were of the open angle type and 46 (34.8{\%}) were of the angle closure type. Twenty-three eyes belong to the secondary glaucoma group. Sixty-five eyes had plain or non-augmented trabeculectomy (TE) while 20 were augmented with mitomycin C (MMC) and 11 with 5 flourouracil (5FU). In 31 eyes the plain TEs were combined with extracapsular cataract extraction (ECCE) and 4 with phacoemusification. One case had combined ECCE and augmented trabeculectomy with mitomycin-C. The patients were followed up at 1 month, 6 months, 1 year and 2 years. Ninety-four of 132 (71.2{\%}) eyes had successful surgery with intraocular pressure (IOP) of less than 21 mmHg (tonometric success) at the end of 2 years. Four of these patients needed topical medication for the IOP control. More failures were seen in patients with cystic blebs than those with diffuse blebs. Complications include hypotony, shallow anterior chamber, cataracts and hyphaema. The majority of cases (53{\%}) had no complications.},
	author = {Mallat, Stephane},
	booktitle = {Elsevier},
	doi = {10.1016/B978-012466606-1/50004-0},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Mallat3.pdf:pdf},
	isbn = {9780123743701},
	issn = {0300-5283},
	keywords = {Signal processing -- Mathematics,Wavelets (Mathematics)},
	number = {3},
	pages = {378--83},
	pmid = {15727384},
	title = {{A Wavelet Tour of Signal Processing The Sparse Way}},
	volume = {59},
	year = {2004}
}

@article{kruskal1977three,
	title={Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics},
	author={Kruskal, Joseph B},
	journal={Linear algebra and its applications},
	volume={18},
	number={2},
	pages={95--138},
	year={1977},
	publisher={Elsevier}
}

@article{donoho2003optimally,
	title={Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization},
	author={Donoho, David L and Elad, Michael},
	journal={Proceedings of the National Academy of Sciences},
	volume={100},
	number={5},
	pages={2197--2202},
	year={2003},
	publisher={National Acad Sciences}
}

@article{Elad2010,
	abstract = {This textbook introduces sparse and redundant representations with a focus on applications in signal and image processing. The theoretical and numerical foundations are tackled before the applications are discussed. Mathematical modeling for signal sources is discussed along with how to use the proper model for tasks such as denoising, restoration, separation, interpolation and extrapolation, compression, sampling, analysis and synthesis, detection, recognition, and more. The presentation is elegant and engaging. Sparse and Redundant Representations is intended for graduate students in applied mathematics and electrical engineering, as well as applied mathematicians, engineers, and researchers who are active in the fields of signal and image processing.},
	archivePrefix = {arXiv},
	arxivId = {g},
	author = {Elad, Michael},
	doi = {10.1007/978-1-4419-7011-4},
	eprint = {g},
	isbn = {9781441970107},
	issn = {144197010X},
	journal = {Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing},
	pages = {1--376},
	pmid = {16351060},
	title = {{Sparse and redundant representations: From theory to applications in signal and image processing}},
	year = {2010}
}

@article{Pereira2014,
	abstract = {Compressive Sensing (CS) allows for reconstructing sparse signals within a low acceptable error using less measurements than stipulated by the Nyquist criterion. This CS paradigm rests on the assumption that there is a basis in which the signal is sparse, and one employs random measurements by means of projections on a sensing matrix reconstructing the signal from these measurements through l1-norm minimization on the sparsity basis. In this work, we propose a method to design sensing matrices with minimum coherence to a given sparsifying orthogonal basis. We provide a mathematical proof of the optimality in terms of coherence minimization for the proposed sensing matrices. This result is extended for biorthogonal bases in order to provide sensing matrices with low coherence, that have advantages when compared to Noiselets in a CS paradigm. Experimental results in an image compression setup show that the proposed sensing matrices provide superior rate-distortion results than Noiselets. These results indicate that the proposed sensing matrices tend to outperform Noiselets when sensing natural images. {\textcopyright} 2014 Elsevier Inc.},
	author = {Pereira, Marcio P. and Lovisolo, Lisandro and {Da Silva}, Eduardo A B and {De Campos}, Marcello L R},
	doi = {10.1016/j.dsp.2014.01.006},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Measurement Matrix Construction/On the design of maximally incoherent sensing matrices for compressed sensing using orthogonal bases and its extension for biorthogonal bases case.pdf:pdf},
	isbn = {1051-2004},
	issn = {10512004},
	journal = {Digital Signal Processing: A Review Journal},
	keywords = {Biorthogonal bases,Compressive sensing,Orthogonal bases,Signal compression,Signal sampling},
	number = {1},
	pages = {12--22},
	publisher = {Elsevier Inc.},
	title = {{On the design of maximally incoherent sensing matrices for compressed sensing using orthogonal bases and its extension for biorthogonal bases case}},
	volume = {27},
	year = {2014}
}

@article{Moreira2014,
	author = {Moreira, Joel},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/RIP-matrix.pdf:pdf},
	pages = {1--4},
	title = {{What is... a rip matrix?}},
	year = {2014}
}

@article{Romberg2013,
	author = {Romberg, Justin},
	title = {{Mathematics of Compressive Sensing : Random matrices are restricted isometries}},
	year = {2013}
	note = {ENS Winter School}
}

@article{CANDES2008589,
	title = "The restricted isometry property and its implications for compressed sensing",
	journal = "Comptes Rendus Mathematique",
	volume = "346",
	number = "9",
	pages = "589 - 592",
	year = "2008",
	issn = "1631-073X",
	doi = "https://doi.org/10.1016/j.crma.2008.03.014",
	author = "Emmanuel J. Candès",
	abstract = "Abstract It is now well-known that one can reconstruct sparse or compressible signals accurately from a very limited number of measurements, possibly contaminated with noise. This technique known as “compressed sensing” or “compressive sampling” relies on properties of the sensing matrix such as the restricted isometry property. In this Note, we establish new results about the accuracy of the reconstruction from undersampled measurements which improve on earlier estimates, and have the advantage of being more elegant. To cite this article: E.J. Candès, C. R. Acad. Sci. Paris, Ser. I 346 (2008).Résumé Il est maintenant bien connu que l'on peut reconstruire des signaux compressibles de manière précise à partir d'un nombre étonnamment petit de mesures, peut-être même bruitées. Cette technique appelée le “compressed sensing” ou “compressive sampling” utilise des propriétés de la matrice d'échantillonage comme la propriété d'isométrie restreinte. Dans cette Note, nous présentons de nouveaux résultats sur la reconstruction de signaux à partir de données incomplètes qui améliorent des travaux précedents et qui, en outre, ont l'avantage d'être plus élégants. Pour citer cet article : E.J. Candès, C. R. Acad. Sci. Paris, Ser. I 346 (2008)."
}

@article{Blanchard2011,
	author = {Jeffrey D. Blanchard and Coralia Cartis and Jared Tanner},
	title = {Compressed Sensing: How Sharp Is the Restricted Isometry Property?},
	journal = {SIAM Review},
	volume = {53},
	number = {1},
	pages = {105-125},
	year = {2011},
	doi = {10.1137/090748160},
}

@article{Adcock2015,
	abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
	archivePrefix = {arXiv},
	arxivId = {1011.1669},
	author = {Adcock, Ben and Hansen, Anders C. and Roman, Bogdan},
	doi = {10.1007/978-3-319-16042-9_5},
	eprint = {1011.1669},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/639a3cacdcddc32f595a68dc46796ae4242e.pdf:pdf},
	isbn = {978-3-319-16041-2},
	issn = {1751-8113},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	number = {8},
	pages = {143--167},
	pmid = {25246403},
	title = {{The Quest for Optimal Sampling: Computationally Efficient, Structure-Exploiting Measurements for Compressed Sensing}},

	volume = {44},
	year = {2015}
}

@article{Roman2014,
	abstract = {This paper demonstrates how new principles of compressed sensing, namely asymptotic incoherence, asymptotic sparsity and multilevel sampling, can be utilised to better understand underlying phenomena in practical compressed sensing and improve results in real-world applications. The contribution of the paper is fourfold: First, it explains how the sampling strategy depends not only on the signal sparsity but also on its structure, and shows how to design effective sampling strategies utilising this. Second, it demonstrates that the optimal sampling strategy and the efficiency of compressed sensing also depends on the resolution of the problem, and shows how this phenomenon markedly affects compressed sensing results and how to exploit it. Third, as the new framework also fits analog (infinite dimensional) models that govern many inverse problems in practice, the paper describes how it can be used to yield substantial improvements. Fourth, by using multilevel sampling, which exploits the structure of the signal, the paper explains how one can outperform random Gaussian/Bernoulli sampling even when the classical {\$}l{\^{}}1{\$} recovery algorithm is replaced by modified algorithms which aim to exploit structure such as model based or Bayesian compressed sensing or approximate message passaging. This final observation raises the question whether universality is desirable even when such matrices are applicable. Examples of practical applications investigated in this paper include Magnetic Resonance Imaging (MRI), Electron Microscopy (EM), Compressive Imaging (CI) and Fluorescence Microscopy (FM). For the latter, a new compressed sensing approach is also presented.},
	archivePrefix = {arXiv},
	arxivId = {1406.4178},
	author = {Roman, Bogdan and Hansen, Anders and Adcock, Ben},
	eprint = {1406.4178},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/On asymptotic structure in compressed sensing.pdf:pdf},
	number = {c},
	pages = {1--10},
	title = {{On asymptotic structure in compressed sensing}},
	year = {2014}
}

@article{Adcock2013,
	abstract = {In this paper we bridge the substantial gap between existing compressed sensing theory and its current use in real-world applications. We do so by introducing a mathematical framework that generalizes the three standard pillars of compressed sensing - namely, sparsity, incoherence and uniform random subsampling - to three new concepts: asymptotic sparsity, asymptotic incoherence and multilevel random sampling. These assumptions are more relevant for many problems; in particular, imaging. As a result, our theory explains the abundance of numerical evidence demonstrating the advantage of so-called variable density sampling strategies in compressive MRI. An important conclusion of our theory is that in applications such as these the success of compressed sensing is resolution dependent. At low resolutions, there is little advantage over classical linear reconstruction. However, the situation changes dramatically once the resolution is increased, in which case compressed sensing can and will offer substantial benefits.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1302.0561v1},
	author = {Adcock, Ben and Hansen, a C and Poon, C and Roman, B},
	eprint = {arXiv:1302.0561v1},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/BreakingCoherenceArxiv (1).pdf:pdf},
	isbn = {2011277906},
	journal = {arXiv preprint arXiv:1302.0561},
	pages = {1--44},
	title = {{Breaking the coherence barrier: asymptotic incoherence and asymptotic sparsity in compressed sensing}},
	year = {2013}
}

@article{Adcock,
	author = {Adcock, Ben and Canada, B C and Hansen, Anders C and Roman, Bogdan},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/a11-adcock.pdf:pdf},
	title = {{Compressed sensing with local structure : theory , applications and benefits}}
}

@article{Candes2011,
	abstract = {This paper introduces a simple and very general theory of compressive sensing. In this theory, the sensing mechanism simply selects sensing vectors independently at random from a probability distribution {\textless}formula formulatype="inline"{\textgreater}{\textless}tex Notation="TeX"{\textgreater}{\$}F{\$}{\textless}/tex{\textgreater}{\textless}/formula{\textgreater}; it includes all standard models{\&}{\#}x2014;e.g., Gaussian, frequency measurements{\&}{\#}x2014;discussed in the literature, but also provides a framework for new measurement strategies as well. We prove that if the probability distribution {\textless}formula formulatype="inline"{\textgreater}{\textless}tex Notation="TeX"{\textgreater}{\$}F{\$}{\textless}/tex{\textgreater}{\textless}/formula{\textgreater} obeys a simple incoherence property and an isotropy property, one can faithfully recover approximately sparse signals from a minimal number of noisy measurements. The novelty is that our recovery results do not require the restricted isometry property (RIP) to hold near the sparsity level in question, nor a random model for the signal. As an example, the paper shows that a signal with {\textless}formula formulatype="inline"{\textgreater}{\textless}tex Notation="TeX"{\textgreater}{\$}s{\$}{\textless}/tex{\textgreater} {\textless}/formula{\textgreater} nonzero entries can be faithfully recovered from about {\textless}formula formulatype="inline"{\textgreater}{\textless}tex Notation="TeX"{\textgreater}{\$}s \backslashlog n{\$}{\textless}/tex{\textgreater}{\textless}/formula{\textgreater} Fourier coefficients that are contaminated with noise.},
	archivePrefix = {arXiv},
	arxivId = {1011.3854},
	author = {Cand{\`{e}}s, Emmanuel J. and Plan, Yaniv},
	doi = {10.1109/TIT.2011.2161794},
	eprint = {1011.3854},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/1011.3854.pdf:pdf},
	isbn = {0018-9448},
	issn = {00189448},
	journal = {IEEE Transactions on Information Theory},
	keywords = {(weak) restricted isometries,Compressed sensing,Dantzig selector,Gross' golfing scheme,LASSO,operator Bernstein inequalities,random matrices,sparse regression,ℓ1 minimization},
	number = {11},
	pages = {7235--7254},
	pmid = {19775966},
	title = {{A probabilistic and RIPless theory of compressed sensing}},
	volume = {57},
	year = {2011}
}

@article{tibshirani1996regression,
	title={Regression shrinkage and selection via the lasso},
	author={Tibshirani, Robert},
	journal={Journal of the Royal Statistical Society. Series B (Methodological)},
	pages={267--288},
	year={1996},
	publisher={JSTOR}
}

@article{Zhang2015,
	abstract = {Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this article is to provide a comprehensive study and an updated review on sparse representation and to supply a guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: sparse representation with {\$}l{\_}0{\$}-norm minimization, sparse representation with {\$}l{\_}p{\$}-norm (0{\$}{\textless}{\$}p{\$}{\textless}{\$}1) minimization, sparse representation with {\$}l{\_}1{\$}-norm minimization and sparse representation with {\$}l{\_}{\{}2,1{\}}{\$}-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: greedy strategy approximation, constrained optimization, proximity algorithm-based optimization, and homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. Specifically, an experimentally comparative study of these sparse representation algorithms was presented. The Matlab code used in this paper can be available at: http://www.yongxu.org/lunwen.html.},
	archivePrefix = {arXiv},
	arxivId = {1602.07017},
	author = {Zhang, Zheng and Xu, Yong and Yang, Jian and Li, Xuelong and Zhang, David},
	doi = {10.1109/ACCESS.2015.2430359},
	eprint = {1602.07017},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/1602.07017.pdf:pdf},
	isbn = {2014041717},
	issn = {21693536},
	journal = {IEEE Access},
	keywords = {Sparse representation,compressive sensing,constrained optimization,dictionary learning,greedy algorithm,homotopy algorithm,proximal algorithm},
	pages = {490--530},
	title = {{A Survey of Sparse Representation: Algorithms and Applications}},
	volume = {3},
	year = {2015}
}

@article{chen2001atomic,
	title={Atomic decomposition by basis pursuit},
	author={Chen, Scott Shaobing and Donoho, David L and Saunders, Michael A},
	journal={SIAM review},
	volume={43},
	number={1},
	pages={129--159},
	year={2001},
	publisher={SIAM}
}

@inproceedings{pati1993orthogonal,
	title={Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition},
	author={Pati, Yagyensh Chandra and Rezaiifar, Ramin and Krishnaprasad, Perinkulam Sambamurthy},
	booktitle={Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on},
	pages={40--44},
	year={1993},
	organization={IEEE}
}

@article{donoho2012sparse,
	title={Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit},
	author={Donoho, David L and Tsaig, Yaakov and Drori, Iddo and Starck, Jean-Luc},
	journal={IEEE Transactions on Information Theory},
	volume={58},
	number={2},
	pages={1094--1121},
	year={2012},
	publisher={IEEE}
}

@article{needell2009cosamp,
	title={CoSaMP: Iterative signal recovery from incomplete and inaccurate samples},
	author={Needell, Deanna and Tropp, Joel A},
	journal={Applied and Computational Harmonic Analysis},
	volume={26},
	number={3},
	pages={301--321},
	year={2009},
	publisher={Elsevier}
}

@article{figueiredo2007gradient,
	title={Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems},
	author={Figueiredo, M{\'a}rio AT and Nowak, Robert D and Wright, Stephen J},
	journal={IEEE Journal of selected topics in signal processing},
	volume={1},
	number={4},
	pages={586--597},
	year={2007},
	publisher={IEEE}
}

@article{efron2004least,
	title={Least angle regression},
	author={Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert and others},
	journal={The Annals of statistics},
	volume={32},
	number={2},
	pages={407--499},
	year={2004},
	publisher={Institute of Mathematical Statistics}
}

@article{portugal2000truncated,
	title={A truncated primal-infeasible dual-feasible network interior point method},
	author={Portugal, Luis F and Resende, Mauricio GC and Veiga, Geraldo and J{\'u}dice, JJ},
	journal={Networks},
	volume={35},
	number={2},
	pages={91--108},
	year={2000},
	publisher={Wiley Online Library}
}

@article{yang2011alternating,
	title={Alternating direction algorithms for $\backslash$ell\_1-problems in compressive sensing},
	author={Yang, Junfeng and Zhang, Yin},
	journal={SIAM journal on scientific computing},
	volume={33},
	number={1},
	pages={250--278},
	year={2011},
	publisher={SIAM}
}

@inproceedings{figueiredo2005bound,
	title={A bound optimization approach to wavelet-based image deconvolution},
	author={Figueiredo, M{\'a}rio AT and Nowak, Robert D},
	booktitle={Image Processing, 2005. ICIP 2005. IEEE International Conference on},
	volume={2},
	pages={II--782},
	year={2005},
	organization={IEEE}
}

@article{beck2009fast,
	title={A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
	author={Beck, Amir and Teboulle, Marc},
	journal={SIAM journal on imaging sciences},
	volume={2},
	number={1},
	pages={183--202},
	year={2009},
	publisher={SIAM}
}

@article{Wright2009,
	abstract = {Finding sparse approximate solutions to large underdetermined linear systems of equations is a common problem in signal/image processing and statistics. Basis pursuit, the least absolute shrinkage and selection operator (LASSO), wavelet-based deconvolution and reconstruction, and compressed sensing (CS) are a few well-known areas in which problems of this type appear. One standard approach is to minimize an objective function that includes a quadratic ({\textless}i{\textgreater}lscr{\textless}/i{\textgreater} {\textless}sub{\textgreater}2{\textless}/sub{\textgreater}) error term added to a sparsity-inducing (usually lscr{\textless}sub{\textgreater}1{\textless}/sub{\textgreater}) regularizater. We present an algorithmic framework for the more general problem of minimizing the sum of a smooth convex function and a nonsmooth, possibly nonconvex regularizer. We propose iterative methods in which each step is obtained by solving an optimization subproblem involving a quadratic term with diagonal Hessian (i.e., separable in the unknowns) plus the original sparsity-inducing regularizer; our approach is suitable for cases in which this subproblem can be solved much more rapidly than the original problem. Under mild conditions (namely convexity of the regularizer), we prove convergence of the proposed iterative algorithm to a minimum of the objective function. In addition to solving the standard lscr{\textless}sub{\textgreater}2{\textless}/sub{\textgreater}-lscr{\textless}sub{\textgreater}1{\textless}/sub{\textgreater} case, our framework yields efficient solution techniques for other regularizers, such as an lscr{\textless}sub{\textgreater}infin{\textless}/sub{\textgreater} norm and group-separable regularizers. It also generalizes immediately to the case in which the data is complex rather than real. Experiments with CS problems show that our approach is competitive with the fastest known methods for the standard lscr{\textless}sub{\textgreater}2{\textless}/sub{\textgreater}-lscr{\textless}sub{\textgreater}1{\textless}/sub{\textgreater} problem, as well as being efficient on problems with other separable regularization terms.},
	author = {Wright, Stephen J. and Nowak, Robert D. and Figueiredo, M{\'{a}}rio A T},
	doi = {10.1109/TSP.2009.2016892},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/SPARSA - Sparse Reconstruction by Separable Approximation.pdf:pdf},
	isbn = {1424414849},
	issn = {1053587X},
	journal = {IEEE Transactions on Signal Processing},
	keywords = {Compressed sensing,Optimization,Reconstruction,Sparse approximation},
	number = {7},
	pages = {2479--2493},
	title = {{Sparse reconstruction by separable approximation}},
	volume = {57},
	year = {2009}
}

@article{becker2011nesta,
	title={NESTA: A fast and accurate first-order method for sparse recovery},
	author={Becker, Stephen and Bobin, J{\'e}r{\^o}me and Cand{\`e}s, Emmanuel J},
	journal={SIAM Journal on Imaging Sciences},
	volume={4},
	number={1},
	pages={1--39},
	year={2011},
	publisher={SIAM}
}

@article{Li2015,
	abstract = {Compressed sensing (CS) enables people to acquire the compressed measurements directly and recover sparse or compressible signals faithfully even when the sampling rate is much lower than the Nyquist rate. However, the pure random sensing matrices usually require huge memory for storage and high computational cost for signal reconstruction. Many structured sensing matrices have been proposed recently to simplify the sensing scheme and the hardware implementation in practice. Based on the restricted isometry property and coherence, couples of existing structured sensing matrices are reviewed in this paper, which have special structures, high recovery performance, and many advantages such as the simple construction, fast calculation and easy hardware implementation. The number of measurements and the universality of different structure matrices are compared.},
	archivePrefix = {arXiv},
	arxivId = {1408.1391},
	author = {Li, Kezhi and Cong, Shuang},
	doi = {10.1007/s11704-015-3326-8},
	eprint = {1408.1391},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/1408.1391.pdf:pdf;:Y$\backslash$:/Literature/Compressive Sensing/Measurement Matrix Construction/State of the art and prospects of structured sensing matrices in compressed sensing.pdf:pdf},
	issn = {20952236},
	journal = {Frontiers of Computer Science},
	keywords = {RIP,coherence,compressed sensing,structured sensing matrices},
	number = {5},
	pages = {665--677},
	title = {{State of the art and prospects of structured sensing matrices in compressed sensing}},
	volume = {9},
	year = {2015}
}

@inproceedings{bajwa2007toeplitz,
	title={Toeplitz-structured compressed sensing matrices},
	author={Bajwa, Waheed U and Haupt, Jarvis D and Raz, Gil M and Wright, Stephen J and Nowak, Robert D},
	booktitle={Statistical Signal Processing, 2007. SSP'07. IEEE/SP 14th Workshop on},
	pages={294--298},
	year={2007},
	organization={IEEE}
}

@article{haupt2010toeplitz,
	title={Toeplitz compressed sensing matrices with applications to sparse channel estimation},
	author={Haupt, Jarvis and Bajwa, Waheed U and Raz, Gil and Nowak, Robert},
	journal={IEEE transactions on information theory},
	volume={56},
	number={11},
	pages={5862--5875},
	year={2010},
	publisher={IEEE}
}

@article{yinpractical,
	title={Practical Compressive Sensing with Toeplitz and Circulant Matrices},
	author={Yin, Wotao and Simon Morgan, B and Yang, Junfeng and others},
	publisher={Citeseer}
}

@article{schacke2013kronecker,
	title={On the Kronecker Product},
	author={Sch{\"a}cke, Kathrin},
	year={2013}
}

@article{Duarte2012,
	abstract = {Compressive sensing (CS) is an emerging approach for the acquisition of signals having a sparse or compressible representation in some basis. While the CS literature has mostly focused on problems involving 1-D signals and 2-D images, many important applications involve multidimensional signals; the construction of sparsifying bases and measurement systems for such signals is complicated by their higher dimensionality. In this paper, we propose the use of Kronecker product matrices in CS for two purposes. First, such matrices can act as sparsifying bases that jointly model the structure present in all of the signal dimensions. Second, such matrices can represent the measurement protocols used in distributed settings. Our formulation enables the derivation of analytical bounds for the sparse approximation of multidimensional signals and CS recovery performance, as well as a means of evaluating novel distributed measurement schemes.},
	author = {Duarte, Marco F. and Baraniuk, Richard G.},
	doi = {10.1109/TIP.2011.2165289},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/05986706.pdf:pdf},
	isbn = {9550071030},
	issn = {10577149},
	journal = {IEEE Transactions on Image Processing},
	keywords = {Compressed sensing,compression algorithms,hyperspectral imaging,multidimensional signal processing,video compression},
	number = {2},
	pages = {494--504},
	pmid = {21859622},
	title = {{Kronecker compressive sensing}},
	volume = {21},
	year = {2012}
}

@article{Rivenson2009,
	abstract = {Compressive imaging (CI) is a natural branch of compressed sensing (CS). Although a number of CI implementations have started to appear, the design of efficient CI system still remains a challenging problem. One of the main difficulties in implementing CI is that it involves huge amounts of data, which has far-reaching implications for the complexity of the optical design, calibration, data storage and computational burden. In this paper, we solve these problems by using a two-dimensional separable sensing operator. By so doing, we reduce the complexity by factor of 10{\textless}sup{\textgreater}6{\textless}/sup{\textgreater} for megapixel images. We show that applying this method requires only a reasonable amount of additional samples.},
	author = {Rivenson, Yair and Stern, Adrian},
	doi = {10.1109/LSP.2009.2017817},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Compressed Imaging With a Separable Sensing Operator.pdf:pdf},
	isbn = {1070-9908},
	issn = {10709908},
	journal = {IEEE Signal Processing Letters},
	keywords = {Compressed sensing,Compressive imaging,Kronecker product,Mutual coherence,Separable operator},
	number = {6},
	pages = {449--452},
	title = {{Compressed imaging with a separable sensing operator}},
	volume = {16},
	year = {2009}
}

@article{starck2002curvelet,
	title={The curvelet transform for image denoising},
	author={Starck, Jean-Luc and Cand{\`e}s, Emmanuel J and Donoho, David L},
	journal={IEEE Transactions on image processing},
	volume={11},
	number={6},
	pages={670--684},
	year={2002},
	publisher={IEEE}
}

@inproceedings{do2002contourlets,
	title={Contourlets: a directional multiresolution image representation},
	author={Do, Minh N and Vetterli, Martin},
	booktitle={Image Processing. 2002. Proceedings. 2002 International Conference on},
	volume={1},
	pages={I--I},
	year={2002},
	organization={IEEE}
}

@article{guo2007optimally,
	title={Optimally sparse multidimensional representation using shearlets},
	author={Guo, Kanghui and Labate, Demetrio},
	journal={SIAM journal on mathematical analysis},
	volume={39},
	number={1},
	pages={298--318},
	year={2007},
	publisher={SIAM}
}

@article{le2005sparse,
	title={Sparse geometric image representations with bandelets},
	author={Le Pennec, Erwan and Mallat, St{\'e}phane},
	journal={IEEE transactions on image processing},
	volume={14},
	number={4},
	pages={423--438},
	year={2005},
	publisher={IEEE}
}

@book{daubechies1992ten,
	title={Ten lectures on wavelets},
	author={Daubechies, Ingrid},
	year={1992},
	publisher={SIAM}
}

@article{olshausen1996emergence,
	title={Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
	author={Olshausen, Bruno A and Field, David J},
	journal={Nature},
	volume={381},
	number={6583},
	pages={607},
	year={1996},
	publisher={Nature Publishing Group}
}

@article{Petrov2012,
	archivePrefix = {arXiv},
	arxivId = {0908.0050},
	author = {Petrov, Yury},
	doi = {10.1561/2200000015},
	eprint = {0908.0050},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/MichalAharonPhDThesis.pdf:pdf},
	isbn = {9781424469840},
	issn = {1053587X},
	journal = {PloS one},
	number = {10},
	pages = {e44439},
	pmid = {1000198949},
	title = {{Overcomplete dictionaries for sparse representation of signals}},
	volume = {7},
	year = {2012}
}

@inproceedings{engan1999method,
	title={Method of optimal directions for frame design},
	author={Engan, Kjersti and Aase, Sven Ole and Husoy, J Hakon},
	booktitle={Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on},
	volume={5},
	pages={2443--2446},
	year={1999},
	organization={IEEE}
}

@article{Aharon2006,
	abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data},
	archivePrefix = {arXiv},
	arxivId = {59749104367},
	author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
	doi = {10.1109/TSP.2006.881199},
	eprint = {59749104367},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/32{\_}KSVD{\_}IEEE{\_}TSP.pdf:pdf},
	isbn = {1053-587X},
	issn = {1053587X},
	journal = {IEEE Transactions on Signal Processing},
	keywords = {Atom decomposition,Basis pursuit,Codebook,Dictionary,FOCUSS,Gain-shape VQ,K-SVD,K-means,Matching pursuit,Sparse representation,Training,Vector quantization},
	number = {11},
	pages = {4311--4322},
	pmid = {22542666},
	title = {{K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation}},
	volume = {54},
	year = {2006}
}

@article{mairal2010online,
	title={Online learning for matrix factorization and sparse coding},
	author={Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
	journal={Journal of Machine Learning Research},
	volume={11},
	number={Jan},
	pages={19--60},
	year={2010}
}

@article{Rubinstein,
	author = {Rubinstein, Ron and Zibulevsky, Michael},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/10.1.1.159.7953.pdf:pdf},
	number = {X},
	pages = {1--11},
	title = {{Learning Sparse Dictionaries for Sparse Signal Representation}},
	volume = {X}
}

@article{Christen2010,
	author = {Christen, Simon and Studer, Christoph and Pope, Graeme},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/SAI{\_}simonch{\_}DLfSR{\_}report{\_}screen.pdf:pdf},
	title = {{Dictionary Learning for Super-Resolution}},
	year = {2010}
}

@article{Kulkarni2016,
	abstract = {The goal of this paper is to present a non-iterative and more importantly an extremely fast algorithm to reconstruct images from compressively sensed (CS) random measurements. To this end, we propose a novel convolutional neural network (CNN) architecture which takes in CS measurements of an image as input and outputs an intermediate reconstruction. We call this network, ReconNet. The intermediate reconstruction is fed into an off-the-shelf denoiser to obtain the final reconstructed image. On a standard dataset of images we show significant improvements in reconstruction results (both in terms of PSNR and time complexity) over state-of-the-art iterative CS reconstruction algorithms at various measurement rates. Further, through qualitative experiments on real data collected using our block single pixel camera (SPC), we show that our network is highly robust to sensor noise and can recover visually better quality images than competitive algorithms at extremely low sensing rates of 0.1 and 0.04. To demonstrate that our algorithm can recover semantically informative images even at a low measurement rate of 0.01, we present a very robust proof of concept real-time visual tracking application.},
	archivePrefix = {arXiv},
	arxivId = {1601.06892},
	author = {Kulkarni, Kuldeep and Lohit, Suhas and Turaga, Pavan and Kerviche, Ronan and Ashok, Amit},
	doi = {10.1109/CVPR.2016.55},
	eprint = {1601.06892},
	file = {:Y$\backslash$:/Literature/Deep Learning/1601.06892.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {10636919},
	title = {{ReconNet: Non-Iterative Reconstruction of Images from Compressively Sensed Random Measurements}},
	year = {2016}
}

@inproceedings{Mousavi2017,
	abstract = {The promise of compressive sensing (CS) has been offset by two significant challenges. First, real-world data is not exactly sparse in a fixed basis. Second, current high-performance recovery algorithms are slow to converge, which limits CS to either non-real-time applications or scenarios where massive back-end computing is available. In this paper, we attack both of these challenges head-on by developing a new signal recovery framework we call {\{}$\backslash$em DeepInverse{\}} that learns the inverse transformation from measurement vectors to signals using a {\{}$\backslash$em deep convolutional network{\}}. When trained on a set of representative images, the network learns both a representation for the signals (addressing challenge one) and an inverse map approximating a greedy or convex recovery algorithm (addressing challenge two). Our experiments indicate that the DeepInverse network closely approximates the solution produced by state-of-the-art CS recovery algorithms yet is hundreds of times faster in run time. The tradeoff for the ultrafast run time is a computationally intensive, off-line training procedure typical to deep networks. However, the training needs to be completed only once, which makes the approach attractive for a host of sparse recovery problems.},
	archivePrefix = {arXiv},
	arxivId = {1701.03891},
	author = {Mousavi, Ali and Baraniuk, Richard G},
	booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	doi = {10.1109/ICASSP.2017.7952561},
	eprint = {1701.03891},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/1701.03891.pdf:pdf},
	isbn = {978-1-5090-4117-6},
	issn = {15206149},
	month = {mar},
	pages = {2272--2276},
	publisher = {IEEE},
	title = {{Learning to invert: Signal recovery via Deep Convolutional Networks}},
	year = {2017}
}

@inproceedings{Mousavi2015,
	abstract = {In this paper, we develop a new framework for sensing and recovering structured signals. In contrast to compressive sensing (CS) systems that employ linear measurements, sparse representations, and computationally complex convex/greedy algorithms, we introduce a deep learning framework that supports both linear and mildly nonlinear measurements, that learns a structured representation from training data, and that efficiently computes a signal estimate. In particular, we apply a stacked denoising autoencoder (SDA), as an unsupervised feature learner. SDA enables us to capture statistical dependencies between the different elements of certain signals and improve signal recovery performance as compared to the CS approach.},
	archivePrefix = {arXiv},
	arxivId = {1508.04065},
	author = {Mousavi, Ali and Patel, Ankit B. and Baraniuk, Richard G.},
	booktitle = {2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	doi = {10.1109/ALLERTON.2015.7447163},
	eprint = {1508.04065},
	file = {:Y$\backslash$:/Literature/Deep Learning/1508.04065.pdf:pdf},
	isbn = {978-1-5090-1824-6},
	month = {sep},
	pages = {1336--1343},
	publisher = {IEEE},
	title = {{A deep learning approach to structured signal recovery}},
	year = {2015}
}

@article{Dong2016,
	abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparsecoding- based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
	archivePrefix = {arXiv},
	arxivId = {1501.00092},
	author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
	doi = {10.1109/TPAMI.2015.2439281},
	eprint = {1501.00092},
	file = {:Y$\backslash$:/Literature/Deep Learning/07115171.pdf:pdf},
	isbn = {0162-8828},
	issn = {01628828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Super-resolution,deep convolutional neural networks,sparse coding},
	number = {2},
	pages = {295--307},
	pmid = {26761735},
	title = {{Image Super-Resolution Using Deep Convolutional Networks}},
	volume = {38},
	year = {2016}
}

@article{nejati2016boosted,
	title={Boosted dictionary learning for image compression},
	author={Nejati, Mansour and Samavi, Shadrokh and Karimi, Nader and Soroushmehr, Sayed Mohammad Reza and Najarian, Kayvan},
	journal={IEEE Transactions on Image Processing},
	volume={25},
	number={10},
	pages={4900--4915},
	year={2016},
	publisher={IEEE}
}

@inproceedings{horev2012adaptive,
	title={Adaptive image compression using sparse dictionaries},
	author={Horev, Inbal and Bryt, Ori and Rubinstein, Ron},
	booktitle={Systems, Signals and Image Processing (IWSSIP), 2012 19th International Conference on},
	pages={592--595},
	year={2012},
	organization={IEEE}
}

@inproceedings{skretting2011image,
	title={Image compression using learned dictionaries by RLS-DLA and compared with K-SVD},
	author={Skretting, Karl and Engan, Kjersti},
	booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
	pages={1517--1520},
	year={2011},
	organization={IEEE}
}

@article{Elad2008,
	abstract = {... Fig. 5. Improvement in the denoising results after each iteration of the K - SVD  algorithm , executed on noisy patches of the image “Peppers.” the value is 34.62 dB. ... [36] M. Aharon, M. Elad, and AM Bruckstein, “The K - SVD : An algo- rithm for designing of overcomplete  ... 
	},
	author = {Elad, Michael and Aharon, Michal},
	doi = {10.1109/TIP.2006.881969},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/elad06.pdf:pdf},
	isbn = {978-1-4244-5237-8},
	issn = {1057-7149},
	journal = {IEEE Transactions on Image Processing},
	number = {12},
	pages = {2008},
	pmid = {17153947},
	title = {{Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries}},
	volume = {15},
	year = {2008}
}

@article{Shen2009,
	author = {Shen, Bin and Hu, Wei and Zhang, Yimin and Zhang, Yu-jin},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/0838decdc8fcfa8ffeb759ee6b8b1ecf7dca.pdf:pdf},
	isbn = {9781479935093},
	journal = {Icassp},
	pages = {159--162},
	title = {{Image inpainting via sparse representation}},
	year = {2009}
}

@article{Mairal2008,
	abstract = {Abstract: A framework for learning multiscale sparse representations of color images and video with overcomplete dictionaries is presented in this paper. Following the single-scale grayscale K-SVD algorithm introduced in [1], which formulates the sparse dictionary ... $\backslash$n},
	author = {Mairal, Julien and Sapiro, Guillermo and Elad, Michael},
	doi = {10.1137/070697653},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/mms08.pdf:pdf},
	isbn = {1612625134},
	issn = {1540-3459},
	journal = {Multiscale Modeling {\&} Simulation},
	keywords = {070697653,10,1137,49m27,62h35,ams subject classifications,denois-,dictionary,doi,image and video processing,ing,inpainting,interpolation,learning,multiscale representation,sparsity},
	number = {1},
	pages = {214--241},
	pmid = {256452200010},
	title = {{Learning Multiscale Sparse Representations for Image and Video Restoration}},
	volume = {7},
	year = {2008}
}

@article{yang2010image,
	title={Image super-resolution via sparse representation},
	author={Yang, Jianchao and Wright, John and Huang, Thomas S and Ma, Yi},
	journal={IEEE transactions on image processing},
	volume={19},
	number={11},
	pages={2861--2873},
	year={2010},
	publisher={IEEE}
}

@article{kim2010single,
	title={Single-image super-resolution using sparse regression and natural image prior},
	author={Kim, Kwang In and Kwon, Younghee},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={32},
	number={6},
	pages={1127--1133},
	year={2010},
	publisher={IEEE}
}

@inproceedings{yang2008image,
	title={Image super-resolution as sparse representation of raw image patches},
	author={Yang, Jianchao and Wright, John and Huang, Thomas and Ma, Yi},
	booktitle={Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
	pages={1--8},
	year={2008},
	organization={IEEE}
}

@inproceedings{Zhang2010,
	author = {Zhang, Qiang and Li, Baoxin},
	booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2010.5539989},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/05539989.pdf:pdf},
	isbn = {978-1-4244-6984-0},
	month = {jun},
	pages = {2691--2698},
	publisher = {IEEE},
	title = {{Discriminative K-SVD for dictionary learning in face recognition}},
	year = {2010}
}

@article{ZhuolinJiang2013,
	author = {{Zhuolin Jiang} and {Zhe Lin} and Davis, L. S.},
	doi = {10.1109/TPAMI.2013.88},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/CVPR2011{\_}LCKSVD{\_}final.pdf:pdf},
	isbn = {9781457703942},
	issn = {0162-8828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	month = {nov},
	number = {11},
	pages = {2651--2664},
	pmid = {24051726},
	title = {{Label Consistent K-SVD: Learning a Discriminative Dictionary for Recognition}},
	volume = {35},
	year = {2013}
}

@article{Wright2009_src,
	abstract = {We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by l{\{}1{\}}-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as Eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.},
	author = {Wright, J. and Yang, A.Y. and Ganesh, A. and Sastry, S.S. and {Yi Ma}},
	doi = {10.1109/TPAMI.2008.79},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/04483511.pdf:pdf},
	isbn = {0162-8828},
	issn = {0162-8828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Biometry,Biometry: methods,Cluster Analysis,Computer-Assisted,Computer-Assisted: methods,Face,Face: anatomy {\&} histology,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique,src},
	mendeley-tags = {src},
	month = {feb},
	number = {2},
	pages = {210--227},
	pmid = {19110489},
	title = {{Robust Face Recognition via Sparse Representation}},
	volume = {31},
	year = {2009}
}

@article{Geethanath2013,
	abstract = {Compressed sensing (CS) is a mathematical framework that reconstructs data from highly undersampled measurements. To gain acceleration in acquisition time, CS has been applied to MRI and has been demonstrated on diverse MRI methods. This review discusses the important requirements to qualify MRI to become an optimal application of CS, namely, sparsity, pseudo-random undersampling, and nonlinear reconstruction. By utilizing concepts of transform sparsity and compression, CS allows acquisition of only the important coefficients of the signal during the acquisition. A priori knowledge of MR images specifically related to transform sparsity is required for the application of CS. In this paper, Section I introduces the fundamentals of CS and the idea of CS as applied to MRI. The requirements for application of CS to MRI is discussed in Section II, while the various acquisition techniques, reconstruction techniques, the advantages of combining CS and parallel imaging, and sampling mask design problems are discussed in Section III. Numerous applications of CS in MRI due to its ability to improve imaging speed are reviewed in section IV. Clinical evaluations of some of the CS applications recently published are discussed in Section V. Section VI provides information on available open source software that could be used for CS implementations.},
	author = {Geethanath, Sairam and Reddy, Rashmi and Konar, Amaresha Shridhar and Imam, Shaikh and Sundaresan, Rajagopalan and {Babu D. R.}, Ramesh and Venkatesan, Ramesh},
	doi = {10.1615/CritRevBiomedEng.2014008058},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/CS{\_}MRI{\_}CBE{\_}2013.pdf:pdf},
	isbn = {0278-940X (Print)$\backslash$r0278-940X (Linking)},
	issn = {0278-940X},
	journal = {Critical reviews in biomedical engineering},
	number = {3},
	pages = {183--204},
	pmid = {24579643},
	title = {{Compressed Sensing MRI: A Review.}},
	volume = {41},
	year = {2013}
}

@article{Reddy2011,
	abstract = {We describe an imaging architecture for compressive video sensing termed programmable pixel compressive camera (P2C2). P2C2 allows us to capture fast phenomena at frame rates higher than the camera sensor. In P2C2, each pixel has an independent shutter that is modulated at a rate higher than the camera frame-rate. The observed intensity at a pixel is an integration of the incoming light modulated by its specific shutter. We propose a reconstruction algorithm that uses the data from P2C2 along with additional priors about videos to perform temporal super-resolution. We model the spatial redundancy of videos using sparse representations and the temporal redundancy using brightness constancy constraints inferred via optical flow. We show that by modeling such spatio-temporal redundancies in a video volume, one can faithfully recover the underlying high-speed video frames from the observed low speed coded video. The imaging architecture and the reconstruction algorithm allows us to achieve temporal super-resolution without loss in spatial resolution. We implement a prototype of P2C2 using an LCOS modulator and recover several videos at 200 fps using a 25 fps camera.},
	author = {Reddy, Dikpal and Veeraraghavan, Ashok and Chellappa, Rama},
	doi = {10.1109/CVPR.2011.5995542},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Compressive Sensing Applications/P2C2 - Programmable pixel compressive camera for high speed imaging.pdf:pdf},
	isbn = {9781457703942},
	issn = {10636919},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	pages = {329--336},
	title = {{P2C2: Programmable pixel compressive camera for high speed imaging}},
	year = {2011}
}

@article{Gupta2010,
	annote = {NULL},
	author = {Gupta, Mohit and Agrawal, Amit and Veeraraghavan, Ashok},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Compressive Sensing Applications/High Speed CS Camera - Flexible Voxels.pdf:pdf},
	journal = {European Conference on Computer Vision (ECCV)},
	pages = {100--114},
	title = {{Flexible Voxels for Motion-Aware Videography}},
	year = {2010}
}

@article{Ashok2010,
	abstract = {Light field imagers such as the plenoptic and the integral imagers inherently measure projections of the four dimensional (4D) light field scalar function onto a two dimensional sensor and therefore, suffer from a spatial vs. angular resolution trade-off. Programmable light field imagers, proposed recently, overcome this spatioangular resolution trade-off and allow high-resolution capture of the (4D) light field function with multiple measurements at the cost of a longer exposure time. However, these light field imagers do not exploit the spatio-angular correlations inherent in the light fields of natural scenes and thus result in photon-inefficient measurements. Here, we describe two architectures for compressive light field imaging that require relatively few photon-efficient measurements to obtain a high-resolution estimate of the light field while reducing the overall exposure time. Our simulation study shows that, compressive light field imagers using the principal component (PC) measurement basis require four times fewer measurements and three times shorter exposure time compared to a conventional light field imager in order to achieve an equivalent light field reconstruction quality.},
	author = {Ashok, Amit and Neifeld, Mark A},
	doi = {10.1117/12.852738},
	file = {:Y$\backslash$:/Literature/Light Field Camera/CSLightFieldSPIEDSS2010.pdf:pdf},
	isbn = {9780819481542},
	issn = {0277786X},
	journal = {Proc. SPIE},
	keywords = {compressive imaging,hadamard,light field,principal component},
	pages = {76900Q},
	title = {{Compressive light field imaging}},
	volume = {7690},
	year = {2010}
}


@article{Babacan2012,
	abstract = {We propose a novel design for light field image acquisition based on compressive sensing principles. By placing a randomly coded mask at the aperture of a camera, incoherent measurements of the light passing through different parts of the lens are encoded in the captured images. Each captured image is a random linear combination of different angular views of a scene. The encoded images are then used to recover the original light field image via a novel Bayesian reconstruction algorithm. Using the principles of compressive sensing, we show that light field images with a large number of angular views can be recovered from only a few acquisitions. Moreover, the proposed acquisition and recovery method provides light field images with high spatial resolution and signal-to-noise-ratio, and therefore is not affected by limitations common to existing light field camera designs. We present a prototype camera design based on the proposed framework by modifying a regular digital camera. Finally, we demonstrate the effectiveness of the proposed system using experimental results with both synthetic and real images.},
	author = {Babacan, S. Derin and Ansorge, Reto and Luessi, Martin and Mataran, Pablo Ruiz and Molina, Rafael and Katsaggelos, Aggelos K.},
	doi = {10.1109/TIP.2012.2210237},
	file = {:Y$\backslash$:/Literature/Light Field Camera/LIghtfield{\_}derin{\_}ip2012 (1).pdf:pdf},
	isbn = {1057-7149},
	issn = {10577149},
	journal = {IEEE Transactions on Image Processing},
	keywords = {Bayesian methods,coded aperture,compressive sensing,computational photography,image reconstruction,light fields},
	number = {12},
	pages = {4746--4757},
	pmid = {22851261},
	title = {{Compressive light field sensing}},
	volume = {21},
	year = {2012}
}

@article{Marwah2013,
	abstract = {Light ﬁeld photography has gained a signiﬁcant research interest in the last two decades; today, commercial light ﬁeld cameras are widely available. Nevertheless, most existing acquisition approaches either multiplex a low-resolution light ﬁeld into a single 2D sensor image or require multiple photographs to be taken for acquiring a high-resolution light ﬁeld. We propose a compressive light ﬁeld camera architecture that allows for higher-resolution light ﬁelds to be recovered than previously possible from a single image. The proposed architecture comprises three key components: light ﬁeld atoms as a sparse representation of natural light ﬁelds, an optical design that allows for capturing optimized 2D light ﬁeld projections, and robust sparse reconstruction methods to recover a 4D light ﬁeld from a single coded 2D projection. In addition, we demonstrate a variety of other applications for light ﬁeld atoms and sparse coding techniques, including 4D light ﬁeld compression and denoising.},
	author = {Marwah, Kshitij and Wetzstein, Gordon and Bando, Yosuke and Raskar, Ramesh},
	doi = {10.1145/2461912.2461914},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Compressive Sensing Applications/Compressive Light Field Photography.pdf:pdf},
	isbn = {0730-0301},
	issn = {07300301},
	journal = {ACM Transactions on Graphics},
	keywords = {algorithms,compressive,computational photography,experimentation,light fields,sensing},
	number = {4},
	pages = {1},
	title = {{Compressive light field photography using overcomplete dictionaries and optimized projections}},
	volume = {32},
	year = {2013}
}

@article{Tsai2015,
	abstract = {This Letter presents a compressive camera that integrates mechanical translation and spectral dispersion to compress a multi-spectral, high-speed scene onto a monochrome, video-rate detector. Experimental reconstructions of 17 spectral channels and 11 temporal channels from a single measurement are reported for a megapixel-scale monochrome camera.},
	author = {Tsai, Tsung-Han and Llull, Patrick and Yuan, Xin and Carin, Lawrence and Brady, David J.},
	doi = {10.1364/OL.40.004054},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Compressive Sensing Applications/Spectral-temporal compressive imaging.pdf:pdf},
	issn = {1539-4794},
	journal = {Optics Letters},
	keywords = {Computational imaging,Multispectral and hyperspectral imaging,Time imaging},
	number = {17},
	pages = {4054--4057},
	title = {{Spectral-temporal compressive imaging}},
	volume = {40},
	year = {2015}
}

@article{Llull2013,
	abstract = {We use mechanical translation of a coded aperture for code division multiple access compression of video. We discuss the compressed video's temporal resolution and present experimental results for reconstructions of {\textgreater} 10 frames of temporal data per coded snapshot.},
	archivePrefix = {arXiv},
	arxivId = {1302.2575},
	author = {Llull, Patrick and Liao, Xuejun and Yuan, Xin and Yang, Jianbo and Kittle, David and Carin, Lawrence and Sapiro, Guillermo and Brady, David J.},
	doi = {10.1364/OE.21.010526},
	eprint = {1302.2575},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Coded aperture compressive temporal imaging.pdf:pdf},
	isbn = {1094-4087},
	issn = {1094-4087},
	journal = {Optics Express},
	number = {9},
	pages = {10526},
	pmid = {23669910},
	title = {{Coded aperture compressive temporal imaging}},
	volume = {21},
	year = {2013}
}

@article{Llull2014,
	abstract = {We present a prototype imaging system that utilizes focal and temporal image space modulation to compress volumetric spatiotemporal information into planar images. Videos and extended depth of field reconstructions from single compressive measurements are reported.},
	author = {Llull, Patrick and Yuan, Xin and Liao, Xuejun and Yang, Jianbo and Carin, Lawrence and Sapiro, Guillermo and Brady, David J},
	doi = {10.1364/COSI.2014.CM2D.3},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Compressive Sensing Applications/Compressive extended depth of field using image space coding.pdf:pdf},
	journal = {Class. Opt. 2014},
	keywords = {Apertures,Computational imaging},
	pages = {CM2D.3},
	title = {{Compressive extended depth of field using image space coding}},
	year = {2014}
}

@article{Rubinstein2010,
	abstract = {An efficient and flexible dictionary structure is proposed for sparse and redundant signal representation. The proposed sparse dictionary is based on a sparsity model of the dictionary atoms over a base dictionary, and takes the form D = ?? A, where ?? is a fixed base dictionary and A is sparse. The sparse dictionary provides efficient forward and adjoint operators, has a compact representation, and can be effectively trained from given example data. In this, the sparse structure bridges the gap between implicit dictionaries, which have efficient implementations yet lack adaptability, and explicit dictionaries, which are fully adaptable but non-efficient and costly to deploy. In this paper, we discuss the advantages of sparse dictionaries, and present an efficient algorithm for training them. We demonstrate the advantages of the proposed structure for 3-D image denoising.},
	author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
	doi = {10.1109/TSP.2009.2036477},
	file = {:Y$\backslash$:/Literature/Dictionary Learning/sparsedict.pdf:pdf},
	isbn = {1053-587X},
	issn = {1053587X},
	journal = {IEEE Transactions on Signal Processing},
	keywords = {Computed tomography,Dictionary learning,K-SVD,Signal denoising,Sparse coding,Sparse representation},
	number = {3 PART 2},
	pages = {1553--1564},
	title = {{Double sparsity: Learning sparse dictionaries for sparse signal approximation}},
	volume = {58},
	year = {2010}
}

@inproceedings{qi2011using,
	title={Using the kernel trick in compressive sensing: Accurate signal recovery from fewer measurements},
	author={Qi, Hanchao and Hughes, Shannon},
	booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
	pages={3940--3943},
	year={2011},
	organization={IEEE}
}


@Misc{cs_matrices_url,
	url = {http://baspfrontiers.epfl.ch/context.php},
	note = {{V}isited 17.10.2017.}
}

@article{duffin1952class,
	title={A class of nonharmonic Fourier series},
	author={Duffin, Richard J and Schaeffer, Albert C},
	journal={Transactions of the American Mathematical Society},
	volume={72},
	number={2},
	pages={341--366},
	year={1952},
	publisher={JSTOR}
}

@article{Koller2015,
	abstract = {We present a prototype compressive video camera that encodes scene movement using a translated binary photomask in the optical path. The encoded recording can then be used to reconstruct multiple output frames from each captured image, effectively synthesizing high speed video. The use of a printed binary mask allows reconstruction at higher spatial resolutions than has been previously demonstrated. In addition, we improve upon previous work by investigating tradeoffs in mask design and reconstruction algorithm selection. We identify a mask design that consistently provides the best performance across multiple reconstruction strategies in simulation, and verify it with our prototype hardware. Finally, we compare reconstruction algorithms and identify the best choice in terms of balancing reconstruction quality and speed.},
	author = {Koller, Roman and Schmid, Lukas and Matsuda, Nathan and Niederberger, Thomas and Spinoulas, Leonidas and Cossairt, Oliver and Schuster, Guido and Katsaggelos, Aggelos K.},
	doi = {10.1364/OE.23.015992},
	file = {:Y$\backslash$:/Literature/Compressive Sensing/Compressive Sensing Applications/High spatio-temporal resolution video with compressive sensing.pdf:pdf},
	issn = {1094-4087},
	journal = {Optics Express},
	number = {12},
	pages = {15992},
	pmid = {26193574},
	title = {{High spatio-temporal resolution video with compressed sensing}},
	volume = {23},
	year = {2015}
}
